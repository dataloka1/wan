import base64
import json
import os
import shutil
import subprocess
import time
import urllib.request
import urllib.parse
import uuid
import websocket
import glob
from pathlib import Path
from typing import Dict, Optional, List, Union
from concurrent.futures import ThreadPoolExecutor, as_completed

from modal import App, Image, Volume, Secret, asgi_app, enter, method
from fastapi import FastAPI, HTTPException, Body
from pydantic import BaseModel, Field

COMFYUI_PATH = Path("/root/comfy/ComfyUI")
MODEL_PATH = Path("/root/comfy/ComfyUI/models")
CACHE_PATH = Path("/cache")

MIN_FILE_SIZE_KB = 500
MIN_LORA_SIZE_KB = 100
MAX_BASE64_SIZE = 100 * 1024 * 1024
MAX_GENERATION_TIME = 3600
SERVER_HOST = "0.0.0.0"
SERVER_PORT = 8000
SERVER_STARTUP_TIMEOUT = 120

MODEL_REGISTRY = {
    "diffusion_models": {
        "wan2.2_t2v_high_noise_14B_fp8_scaled.safetensors": {
            "repo_id": "Comfy-Org/Wan_2.2_ComfyUI_Repackaged",
            "filename": "split_files/diffusion_models/wan2.2_t2v_high_noise_14B_fp8_scaled.safetensors"
        },
        "wan2.2_t2v_low_noise_14B_fp8_scaled.safetensors": {
            "repo_id": "Comfy-Org/Wan_2.2_ComfyUI_Repackaged",
            "filename": "split_files/diffusion_models/wan2.2_t2v_low_noise_14B_fp8_scaled.safetensors"
        },
        "wan2.2_i2v_high_noise_14B_fp8_scaled.safetensors": {
            "repo_id": "Comfy-Org/Wan_2.2_ComfyUI_Repackaged",
            "filename": "split_files/diffusion_models/wan2.2_i2v_high_noise_14B_fp8_scaled.safetensors"
        },
        "wan2.2_i2v_low_noise_14B_fp8_scaled.safetensors": {
            "repo_id": "Comfy-Org/Wan_2.2_ComfyUI_Repackaged",
            "filename": "split_files/diffusion_models/wan2.2_i2v_low_noise_14B_fp8_scaled.safetensors"
        },
        "Wan2_2-Animate-14B_fp8_e4m3fn_scaled_KJ.safetensors": {
            "repo_id": "Kijai/WanVideo_comfy_fp8_scaled",
            "filename": "Wan22Animate/Wan2_2-Animate-14B_fp8_e4m3fn_scaled_KJ.safetensors"
        },
    },
    "vae": {
        "wan_2.1_vae.safetensors": {
            "repo_id": "Comfy-Org/Wan_2.2_ComfyUI_Repackaged",
            "filename": "split_files/vae/wan_2.1_vae.safetensors"
        },
    },
    "text_encoders": {
        "umt5_xxl_fp8_e4m3fn_scaled.safetensors": {
            "repo_id": "Comfy-Org/Wan_2.1_ComfyUI_repackaged",
            "filename": "split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors"
        },
    },
    "clip_vision": {
        "clip_vision_h.safetensors": {
            "repo_id": "Comfy-Org/Wan_2.1_ComfyUI_repackaged",
            "filename": "split_files/clip_vision/clip_vision_h.safetensors"
        },
    },
    "loras": {
        "wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors": {
            "repo_id": "Comfy-Org/Wan_2.2_ComfyUI_Repackaged",
            "filename": "split_files/loras/wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors"
        },
        "wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors": {
            "repo_id": "Comfy-Org/Wan_2.2_ComfyUI_Repackaged",
            "filename": "split_files/loras/wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors"
        },
        "wan2.2_t2v_lightx2v_4steps_lora_v1.1_high_noise.safetensors": {
            "repo_id": "Comfy-Org/Wan_2.2_ComfyUI_Repackaged",
            "filename": "split_files/loras/wan2.2_t2v_lightx2v_4steps_lora_v1.1_high_noise.safetensors"
        },
        "wan2.2_t2v_lightx2v_4steps_lora_v1.1_low_noise.safetensors": {
            "repo_id": "Comfy-Org/Wan_2.2_ComfyUI_Repackaged",
            "filename": "split_files/loras/wan2.2_t2v_lightx2v_4steps_lora_v1.1_low_noise.safetensors"
        },
        "lightx2v_I2V_14B_480p_cfg_step_distill_rank64_bf16.safetensors": {
            "repo_id": "Kijai/WanVideo_comfy",
            "filename": "Lightx2v/lightx2v_I2V_14B_480p_cfg_step_distill_rank64_bf16.safetensors"
        },
        "WanAnimate_relight_lora_fp16.safetensors": {
            "repo_id": "Kijai/WanVideo_comfy",
            "filename": "LoRAs/Wan22_relight/WanAnimate_relight_lora_fp16.safetensors"
        },
        "v2_lora_ZoomIn.safetensors": {
            "repo_id": "guoyww/animatediff",
            "filename": "motion_lora_v2/v2_lora_ZoomIn.safetensors"
        },
        "v2_lora_ZoomOut.safetensors": {
            "repo_id": "guoyww/animatediff",
            "filename": "motion_lora_v2/v2_lora_ZoomOut.safetensors"
        },
        "v2_lora_PanLeft.safetensors": {
            "repo_id": "guoyww/animatediff",
            "filename": "motion_lora_v2/v2_lora_PanLeft.safetensors"
        },
        "v2_lora_PanRight.safetensors": {
            "repo_id": "guoyww/animatediff",
            "filename": "motion_lora_v2/v2_lora_PanRight.safetensors"
        },
        "v2_lora_TiltUp.safetensors": {
            "repo_id": "guoyww/animatediff",
            "filename": "motion_lora_v2/v2_lora_TiltUp.safetensors"
        },
        "v2_lora_TiltDown.safetensors": {
            "repo_id": "guoyww/animatediff",
            "filename": "motion_lora_v2/v2_lora_TiltDown.safetensors"
        },
        "v2_lora_RollingClockwise.safetensors": {
            "repo_id": "guoyww/animatediff",
            "filename": "motion_lora_v2/v2_lora_RollingClockwise.safetensors"
        },
        "v2_lora_RollingAnticlockwise.safetensors": {
            "repo_id": "guoyww/animatediff",
            "filename": "motion_lora_v2/v2_lora_RollingAnticlockwise.safetensors"
        },
    }
}

def _validate_and_decode_base64(data: str, data_type: str = "image") -> str:
    if not data:
        raise ValueError(f"{data_type} data is empty")
    if len(data) > MAX_BASE64_SIZE:
        raise ValueError(f"{data_type} data too large. Max {MAX_BASE64_SIZE/(1024*1024):.2f}MB")
    if data.startswith(f'data:{data_type}/') or data.startswith('data:audio/'):
        if ';base64,' not in data:
            raise ValueError(f"Invalid base64 {data_type} format")
        data = data.split(';base64,')[1]
    try:
        data += '=' * (-len(data) % 4)
        base64.b64decode(data, validate=True)
    except Exception as e:
        raise ValueError(f"Invalid base64 encoding: {str(e)}")
    return data

def _save_base64_to_file(data_base64: str, temp_filename: str, data_type: str = "image") -> str:
    clean_b64 = _validate_and_decode_base64(data_base64, data_type)
    try:
        file_data = base64.b64decode(clean_b64)
        with open(temp_filename, "wb") as f:
            f.write(file_data)
        print(f"[{data_type.upper()}] Saved: {temp_filename} ({len(file_data)/1024:.2f} KB)")
        return temp_filename
    except Exception as e:
        raise ValueError(f"Failed to save {data_type}: {str(e)}")

def _download_model_safe(repo_id: str, filename: str, target_path: Path, model_type: str) -> bool:
    from huggingface_hub import hf_hub_download
    try:
        print(f"[{model_type.upper()}] Downloading: {filename}")
        cached_path = hf_hub_download(
            repo_id=repo_id,
            filename=filename,
            cache_dir=str(CACHE_PATH),
        )
        cached_file = Path(cached_path)
        if not cached_file.exists():
            print(f"[{model_type.upper()}] ERROR: Downloaded file doesn't exist")
            return False
        file_size_kb = cached_file.stat().st_size / 1024
        min_size = MIN_LORA_SIZE_KB if model_type == "loras" else MIN_FILE_SIZE_KB
        if file_size_kb < min_size:
            print(f"[{model_type.upper()}] ERROR: File too small ({file_size_kb:.2f} KB < {min_size} KB)")
            try:
                cached_file.unlink()
            except:
                pass
            return False
        subprocess.run(
            f"ln -sf {cached_path} {target_path}",
            shell=True,
            check=True,
        )
        print(f"[{model_type.upper()}] ✓ Linked: {filename} ({file_size_kb:.2f} KB)")
        return True
    except Exception as e:
        print(f"[{model_type.upper()}] ✗ Failed: {filename} - {str(e)}")
        return False

def setup_comfyui():
    print("\n" + "="*80)
    print("MODEL DOWNLOAD STARTED")
    print("="*80)
    stats = {"total": 0, "success": 0, "failed": 0}
    for model_type, models in MODEL_REGISTRY.items():
        print(f"\n[{model_type.upper()}] Processing {len(models)} models...")
        target_dir = MODEL_PATH / model_type
        target_dir.mkdir(parents=True, exist_ok=True)
        for filename, source in models.items():
            stats["total"] += 1
            target_path = target_dir / filename
            if target_path.exists() and target_path.is_symlink():
                try:
                    file_size = target_path.stat().st_size / 1024
                    min_size = MIN_LORA_SIZE_KB if model_type == "loras" else MIN_FILE_SIZE_KB
                    if file_size > min_size:
                        print(f"[{model_type.upper()}] Already exists: {filename}")
                        stats["success"] += 1
                        continue
                    else:
                        target_path.unlink()
                except:
                    target_path.unlink()
            if _download_model_safe(
                source["repo_id"],
                source["filename"],
                target_path,
                model_type
            ):
                stats["success"] += 1
            else:
                stats["failed"] += 1
    print("\n" + "="*80)
    print(f"MODEL DOWNLOAD COMPLETE: {stats['success']}/{stats['total']} successful, {stats['failed']} failed")
    print("="*80 + "\n")

cache_volume = Volume.from_name("hf-hub-cache", create_if_missing=True)

comfy_image = (
    Image.debian_slim(python_version="3.12")
    .apt_install("git", "ffmpeg", "libgl1-mesa-glx", "libglib2.0-0")
    .pip_install("comfy-cli==1.4.1", force_build=True)
    .run_commands(
        "comfy --skip-prompt install --fast-deps --nvidia --version 0.3.59",
        force_build=True,
    )
    .run_commands(
        "comfy node install comfyui_controlnet_aux",
        "comfy node install ComfyUI-KJNodes",
        "comfy node install ComfyUI-segment-anything-2",
        "comfy node install ComfyUI-WanVideoWrapper",
        "comfy node install ComfyUI-VideoHelperSuite",
        force_build=True,
    )
    .pip_install(
        "huggingface_hub[hf_transfer]>=0.34.0,<1.0",
        "websocket-client",
        "fastapi",
        "uvicorn[standard]",
    )
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1"})
    .run_function(
        setup_comfyui,
        volumes={str(CACHE_PATH): cache_volume},
    )
)

app = App("comfyui-wan2-2-complete-production")

@app.cls(
    image=comfy_image,
    gpu="L40S",
    volumes={str(CACHE_PATH): cache_volume},
    timeout=7200,
    keep_warm=1,
)
class ComfyUI:
    @enter()
    def startup(self):
        print("\n" + "="*80)
        print("COMFYUI STARTUP")
        print("="*80)
        cmd = ["comfy", "launch", "--", "--listen", SERVER_HOST, "--port", str(SERVER_PORT)]
        self.proc = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        print(f"[SERVER] Started with PID: {self.proc.pid}")
        for i in range(SERVER_STARTUP_TIMEOUT):
            try:
                urllib.request.urlopen(f"http://127.0.0.1:{SERVER_PORT}/queue", timeout=5).read()
                print(f"[SERVER] Ready after {i+1} seconds\n")
                return
            except Exception:
                if i % 10 == 0:
                    print(f"[SERVER] Waiting... ({i}/{SERVER_STARTUP_TIMEOUT}s)")
                time.sleep(1)
        raise RuntimeError("ComfyUI failed to start")

    def _queue_prompt(self, client_id: str, prompt_workflow: dict) -> str:
        payload = {"prompt": prompt_workflow, "client_id": client_id}
        payload_json = json.dumps(payload)
        print(f"[QUEUE] Sending workflow ({len(payload_json)/1024:.2f} KB)")
        req = urllib.request.Request(
            f"http://127.0.0.1:{SERVER_PORT}/prompt",
            data=payload_json.encode('utf-8'),
            headers={'Content-Type': 'application/json'}
        )
        try:
            with urllib.request.urlopen(req, timeout=30) as response:
                result = json.loads(response.read())
                if 'error' in result:
                    error_detail = result.get('error', {})
                    print(f"[QUEUE] ERROR: {json.dumps(error_detail, indent=2)}")
                    raise RuntimeError(f"ComfyUI Error: {error_detail}")
                if 'prompt_id' not in result:
                    raise RuntimeError(f"Invalid response: {result}")
                print(f"[QUEUE] Prompt ID: {result['prompt_id']}")
                return result['prompt_id']
        except urllib.error.HTTPError as e:
            error_body = e.read().decode('utf-8')
            print(f"[QUEUE] HTTP {e.code}: {error_body}")
            raise RuntimeError(f"ComfyUI API Error {e.code}: {error_body}")

    def _get_history(self, prompt_id: str) -> dict:
        url = f"http://127.0.0.1:{SERVER_PORT}/history/{prompt_id}"
        with urllib.request.urlopen(url) as response:
            return json.loads(response.read())

    def _get_file(self, filename: str, subfolder: str, folder_type: str) -> bytes:
        params = urllib.parse.urlencode({
            'filename': filename,
            'subfolder': subfolder,
            'type': folder_type
        })
        url = f"http://127.0.0.1:{SERVER_PORT}/view?{params}"
        with urllib.request.urlopen(url) as response:
            data = bytearray()
            while True:
                chunk = response.read(8192)
                if not chunk:
                    break
                data.extend(chunk)
        print(f"[FILE] Downloaded: {len(data)/1024/1024:.2f} MB")
        return bytes(data)

    def _get_video_from_filesystem(self, prompt_id: str, prefix: str = "video") -> bytes:
        """Fallback: directly check output directory for video files"""
        output_dir = COMFYUI_PATH / "output"
        print(f"[FILESYSTEM] Searching in: {output_dir}")
        
        # Wait a bit for file system to sync
        time.sleep(2)
        
        # Try multiple patterns
        patterns = [
            str(output_dir / f"**/{prefix}*.mp4"),
            str(output_dir / f"**/{prefix}*.webm"),
            str(output_dir / f"**/*.mp4"),
            str(output_dir / f"**/*.webm"),
        ]
        
        all_files = []
        for pattern in patterns:
            files = glob.glob(pattern, recursive=True)
            all_files.extend(files)
        
        if not all_files:
            raise ValueError(f"No video files found in {output_dir}")
        
        # Get most recent file
        latest = max(all_files, key=os.path.getctime)
        file_age = time.time() - os.path.getctime(latest)
        
        print(f"[FILESYSTEM] Found: {latest} (age: {file_age:.1f}s)")
        
        with open(latest, 'rb') as f:
            data = f.read()
        
        print(f"[FILESYSTEM] Loaded: {len(data)/1024/1024:.2f} MB")
        return data

    def _get_video_from_websocket(self, prompt_id: str, client_id: str) -> bytes:
        ws_url = f"ws://127.0.0.1:{SERVER_PORT}/ws?clientId={client_id}"
        ws = None
        try:
            ws = websocket.WebSocket()
            ws.connect(ws_url, timeout=10)
            ws.settimeout(60)
            print("[WS] Connected")
        except Exception as e:
            raise RuntimeError(f"WebSocket connection failed: {str(e)}")
        
        start_time = time.time()
        generation_done = False
        
        try:
            while time.time() - start_time < MAX_GENERATION_TIME:
                try:
                    out = ws.recv()
                    if isinstance(out, str):
                        message = json.loads(out)
                        if message.get('type') == 'progress':
                            data = message.get('data', {})
                            value = data.get('value', 0)
                            max_val = max(data.get('max', 1), 1)
                            pct = (value / max_val) * 100
                            print(f"[WS] Progress: {value}/{max_val} ({pct:.1f}%)")
                        elif message.get('type') == 'executing':
                            if not message.get('data', {}).get('node'):
                                print("[WS] Generation signal received, polling for output...")
                                generation_done = True
                                break
                except websocket.WebSocketTimeoutException:
                    continue
        finally:
            if ws:
                ws.close()
        
        if not generation_done and (time.time() - start_time >= MAX_GENERATION_TIME):
            raise TimeoutError(f"Generation timeout ({MAX_GENERATION_TIME}s)")
        
        # Enhanced polling with better diagnostics
        POLL_TIMEOUT = 120
        poll_start_time = time.time()
        
        while time.time() - poll_start_time < POLL_TIMEOUT:
            history = self._get_history(prompt_id)
            
            if prompt_id in history:
                # DEBUG: Print the entire history structure
                print(f"[DEBUG] History keys: {list(history[prompt_id].keys())}")
                
                outputs = history[prompt_id].get('outputs', {})
                print(f"[DEBUG] Output nodes: {list(outputs.keys())}")
                
                # Check ALL possible output locations
                for node_id, node_output in outputs.items():
                    print(f"[DEBUG] Node {node_id} keys: {list(node_output.keys())}")
                    
                    # Try multiple possible keys
                    for video_key in ['videos', 'gifs', 'images', 'outputs']:
                        if video_key in node_output:
                            items = node_output[video_key]
                            if items and len(items) > 0:
                                item = items[0]
                                print(f"[OUTPUT] Found {video_key}: {item.get('filename', 'unknown')}")
                                
                                try:
                                    return self._get_file(
                                        item['filename'],
                                        item.get('subfolder', ''),
                                        item.get('type', 'output')
                                    )
                                except Exception as e:
                                    print(f"[OUTPUT] Failed to retrieve: {e}")
                                    continue
            
            time.sleep(2)
        
        # Better error message with full context
        final_history = self._get_history(prompt_id)
        history_info = json.dumps(final_history.get(prompt_id, {}), indent=2)
        print(f"[DEBUG] Final history:\n{history_info}")
        
        raise ValueError(
            f"No video output found after {POLL_TIMEOUT}s polling. "
            f"Check logs above for history structure."
        )

    def _copy_file_to_comfyui_input(self, base64_data: str, extension: str, data_type: str = "image") -> str:
        temp_file = f"/tmp/{uuid.uuid4()}.{extension}"
        _save_base64_to_file(base64_data, temp_file, data_type)
        input_dir = COMFYUI_PATH / "input"
        input_dir.mkdir(exist_ok=True)
        filename = f"{uuid.uuid4()}.{extension}"
        shutil.copy(temp_file, input_dir / filename)
        return filename

    @method()
    def generate_t2v(
        self,
        prompt: str,
        negative_prompt: str = "",
        width: int = 832,
        height: int = 480,
        num_frames: int = 121,
        steps: int = 30,
        cfg: float = 7.5,
        seed: Optional[int] = None,
        use_fast_mode: bool = False
    ) -> str:
        print(f"\n[T2V] Mode: {'FAST' if use_fast_mode else 'STANDARD'}")
        print(f"[T2V] Prompt: '{prompt[:50]}...'")
        client_id = str(uuid.uuid4())
        
        if seed is None:
            seed = int(time.time() * 1000000) % (2**32)
        
        if use_fast_mode:
            high_noise_model = "wan2.2_t2v_high_noise_14B_fp8_scaled.safetensors"
            low_noise_model = "wan2.2_t2v_low_noise_14B_fp8_scaled.safetensors"
            high_lora = "wan2.2_t2v_lightx2v_4steps_lora_v1.1_high_noise.safetensors"
            low_lora = "wan2.2_t2v_lightx2v_4steps_lora_v1.1_low_noise.safetensors"
            shift_high = 5.0
            shift_low = 5.0
            steps = min(steps, 4)
        else:
            high_noise_model = "wan2.2_t2v_high_noise_14B_fp8_scaled.safetensors"
            low_noise_model = "wan2.2_t2v_low_noise_14B_fp8_scaled.safetensors"
            high_lora = None
            low_lora = None
            shift_high = 8.0
            shift_low = 8.0
        
        workflow = {
            "75": {"class_type": "UNETLoader", "inputs": {"unet_name": high_noise_model, "weight_dtype": "default"}},
            "76": {"class_type": "UNETLoader", "inputs": {"unet_name": low_noise_model, "weight_dtype": "default"}},
            "71": {"class_type": "CLIPLoader", "inputs": {"clip_name": "umt5_xxl_fp8_e4m3fn_scaled.safetensors", "type": "wan", "device": "default"}},
            "73": {"class_type": "VAELoader", "inputs": {"vae_name": "wan_2.1_vae.safetensors"}},
            "72": {"class_type": "CLIPTextEncode", "inputs": {"text": negative_prompt, "clip": ["71", 0]}},
            "89": {"class_type": "CLIPTextEncode", "inputs": {"text": prompt, "clip": ["71", 0]}},
            "74": {"class_type": "EmptyHunyuanLatentVideo", "inputs": {"width": width, "height": height, "length": num_frames, "batch_size": 1}},
        }
        
        if use_fast_mode and high_lora:
            workflow["83"] = {"class_type": "LoraLoaderModelOnly", "inputs": {"lora_name": high_lora, "strength_model": 1.0, "model": ["75", 0]}}
            workflow["85"] = {"class_type": "LoraLoaderModelOnly", "inputs": {"lora_name": low_lora, "strength_model": 1.0, "model": ["76", 0]}}
            high_model_source = ["83", 0]
            low_model_source = ["85", 0]
        else:
            high_model_source = ["75", 0]
            low_model_source = ["76", 0]
        
        workflow.update({
            "82": {"class_type": "ModelSamplingSD3", "inputs": {"shift": shift_high, "model": high_model_source}},
            "86": {"class_type": "ModelSamplingSD3", "inputs": {"shift": shift_low, "model": low_model_source}},
            "81": {"class_type": "KSamplerAdvanced", "inputs": {"add_noise": "enable", "noise_seed": seed, "steps": steps, "cfg": cfg, "sampler_name": "euler", "scheduler": "simple", "start_at_step": 0, "end_at_step": steps // 2, "return_with_leftover_noise": "enable", "model": ["82", 0], "positive": ["89", 0], "negative": ["72", 0], "latent_image": ["74", 0]}},
            "78": {"class_type": "KSamplerAdvanced", "inputs": {"add_noise": "disable", "noise_seed": 0, "steps": steps, "cfg": cfg, "sampler_name": "euler", "scheduler": "simple", "start_at_step": steps // 2, "end_at_step": steps, "return_with_leftover_noise": "disable", "model": ["86", 0], "positive": ["89", 0], "negative": ["72", 0], "latent_image": ["81", 0]}},
            "87": {"class_type": "VAEDecode", "inputs": {"samples": ["78", 0], "vae": ["73", 0]}},
            "88": {"class_type": "CreateVideo", "inputs": {"fps": 16, "images": ["87", 0]}},
            "80": {"class_type": "SaveVideo", "inputs": {"filename_prefix": "t2v", "format": "video/h264-mp4", "video": ["88", 0]}}
        })
        
        prompt_id = self._queue_prompt(client_id, workflow)
        
        try:
            video_bytes = self._get_video_from_websocket(prompt_id, client_id)
        except ValueError as e:
            print(f"[FALLBACK] WebSocket retrieval failed: {e}")
            print("[FALLBACK] Attempting filesystem retrieval...")
            video_bytes = self._get_video_from_filesystem(prompt_id, "t2v")
        
        return base64.b64encode(video_bytes).decode('utf-8')

    @method()
    def generate_i2v(
        self,
        image_base64: str,
        prompt: str,
        negative_prompt: str = "",
        width: int = 1280,
        height: int = 704,
        num_frames: int = 81